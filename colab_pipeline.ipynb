{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Learning Analysis of Smartphone and Electronic-Stethoscope Phonocardiograms for Detection of Reduced Left Ventricular Ejection Fraction\n",
    "\n",
    "This notebook rebuilds all derived artifacts and runs experiments on Google Colab.\n",
    "Default settings prioritize the fastest training (local /content storage for code, data, cache, and results).\n",
    "Set `USE_LOCAL_DATA = False` if your dataset is too large for /content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\nRun cells from top to bottom. The workflow is designed to be safe and reproducible, with optional steps controlled by simple flags (e.g., `RUN_CACHE`).\n\n**Default behavior:**\n- Builds metadata and splits\n- Runs 5-fold CV for model selection\n- Skips caching and final/eval-only runs unless you enable them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 \u2014 Mount Google Drive\nSet `DRIVE_REPO_DIR` to the folder containing this repo and your data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_REPO_DIR = '/content/drive/MyDrive/phonocardiogram-lvef-deeplearning'\n",
    "WORK_DIR = '/content/pcg_repo'\n",
    "DATA_DIR = '/content/pcg_data'\n",
    "RUNS_DIR = '/content/pcg_runs'\n",
    "USE_LOCAL_DATA = True  # fastest default\n",
    "SYNC_BACK_TO_DRIVE = True\n",
    "SYNC_DERIVED = True\n",
    "DRIVE_RUNS_DIR = f\"{DRIVE_REPO_DIR}/runs\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 \u2014 Copy Repo/Data to /content (Fast Mode)\nThis copies the repo (and optionally the dataset) to local Colab storage for speed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "def rsync(src, dst, excludes=None):\n",
    "    cmd = ['rsync', '-a', '--delete']\n",
    "    if excludes:\n",
    "        for ex in excludes:\n",
    "            cmd += ['--exclude', ex]\n",
    "    cmd += [src, dst]\n",
    "    print(' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "rsync(DRIVE_REPO_DIR + '/', WORK_DIR + '/', excludes=['.git', 'cache', 'splits', 'results', 'checkpoints', 'checkpoints_cpu', '__pycache__'])\n",
    "\n",
    "if USE_LOCAL_DATA:\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    rsync(DRIVE_REPO_DIR + '/heart_sounds/', DATA_DIR + '/heart_sounds/')\n",
    "    shutil.copy2(DRIVE_REPO_DIR + '/lvef.csv', DATA_DIR + '/lvef.csv')\n",
    "    LVEF_CSV = f\"{DATA_DIR}/lvef.csv\"\n",
    "    HEART_DIR = f\"{DATA_DIR}/heart_sounds\"\n",
    "else:\n",
    "    LVEF_CSV = f\"{DRIVE_REPO_DIR}/lvef.csv\"\n",
    "    HEART_DIR = f\"{DRIVE_REPO_DIR}/heart_sounds\"\n",
    "\n",
    "if not os.path.exists(LVEF_CSV):\n",
    "    raise FileNotFoundError(f'Missing LVEF CSV: {LVEF_CSV}')\n",
    "if not os.path.isdir(HEART_DIR):\n",
    "    raise FileNotFoundError(f'Missing heart_sounds dir: {HEART_DIR}')\n",
    "\n",
    "os.makedirs(RUNS_DIR, exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "print('WORK_DIR:', WORK_DIR)\n",
    "print('LVEF_CSV:', LVEF_CSV)\n",
    "print('HEART_DIR:', HEART_DIR)\n",
    "print('RUNS_DIR:', RUNS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 \u2014 Install Requirements\nColab already provides PyTorch. This installs the remaining packages from `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "print('torch:', torch.__version__)\n",
    "print('cuda:', torch.version.cuda)\n",
    "print('cuda available:', torch.cuda.is_available())\n",
    "\n",
    "reqs = [r.strip() for r in Path('requirements.txt').read_text().splitlines() if r.strip()]\n",
    "reqs = [r for r in reqs if not r.startswith('torch') and not r.startswith('torchaudio')]\n",
    "cmd = [sys.executable, '-m', 'pip', 'install'] + reqs\n",
    "print('Installing:', ' '.join(reqs))\n",
    "subprocess.run(cmd, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the filename pattern (important)\n",
    "This workflow expects filenames like `aData2001A.wav`, where:\n",
    "- First letter = device code (`a`=android, `i`=iphone, `e`=digital stethoscope)\n",
    "- Digits = patient ID\n",
    "- Last letter = auscultation position (A/P/M/T)\n",
    "\n",
    "If your naming differs, edit `FILENAME_RE` and `DEVICE_MAP` in `src/data/build_metadata.py` before running the next cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 \u2014 Build Metadata\nCreates `metadata.csv` by linking each WAV to patient ID, device, position, and label."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python -m src.data.build_metadata \\\n",
    "  --lvef_csv {LVEF_CSV} \\\n",
    "  --heart_dir {HEART_DIR} \\\n",
    "  --output_csv metadata.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 \u2014 Create Patient-Level Splits\nCreates train/val/test splits and 5-fold CV splits (no patient leakage)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python -m src.data.make_patient_splits \\\n",
    "  --metadata_csv metadata.csv \\\n",
    "  --output_dir splits\n",
    "\n",
    "!python -m src.data.make_patient_cv_splits \\\n",
    "  --metadata_csv metadata.csv \\\n",
    "  --output_dir splits/cv \\\n",
    "  --n_splits 5 \\\n",
    "  --n_repeats 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional \u2014 Global TF Stats (for caching or non-CV runs)\nSet `RUN_GLOBAL_STATS=True` only if you plan to cache tensors or train without CV."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "RUN_GLOBAL_STATS = False  # set True for caching or non-CV training\n",
    "PER_DEVICE_STATS = False  # set True for per-device normalization\n",
    "\n",
    "if RUN_GLOBAL_STATS:\n",
    "    per_device_flag = '--per_device' if PER_DEVICE_STATS else ''\n",
    "\n",
    "    !python -m src.data.compute_stats \\\n",
    "      --train_csv splits/metadata_train.csv \\\n",
    "      --representations mfcc gammatone \\\n",
    "      {per_device_flag}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional \u2014 Precompute Cache\nSet `RUN_CACHE=True` to build cached tensors for faster single-run training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "RUN_CACHE = False  # enable if you plan to train with --use_cache\n",
    "CACHE_ROOT = '/content/pcg_cache'\n",
    "NORMALIZATION = 'per_device' if PER_DEVICE_STATS else 'global'\n",
    "\n",
    "if RUN_CACHE:\n",
    "    if not os.path.exists('tf_stats.json'):\n",
    "        per_device_flag = '--per_device' if PER_DEVICE_STATS else ''\n",
    "        print('Computing tf_stats.json for caching...')\n",
    "        !python -m src.data.compute_stats \\\n",
    "          --train_csv splits/metadata_train.csv \\\n",
    "          --representations mfcc gammatone \\\n",
    "          {per_device_flag}\n",
    "\n",
    "    for rep in ['mfcc', 'gammatone']:\n",
    "        print(f'Caching {rep}...')\n",
    "        !python -m src.data.precompute_cache \\\n",
    "          --representation {rep} \\\n",
    "          --normalization {NORMALIZATION} \\\n",
    "          --cache_root {CACHE_ROOT} \\\n",
    "          --splits splits/metadata_train.csv splits/metadata_val.csv splits/metadata_test.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional \u2014 QA Report\nUse this to audit audio durations, sample rates, and label sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optional QA report\n",
    "# !mkdir -p reports\n",
    "# !python -m src.data.qa_report \\\n",
    "#   --metadata_csv metadata.csv \\\n",
    "#   --output_json reports/qa_report.json \\\n",
    "#   --output_csv reports/qa_records.csv \\\n",
    "#   --fixed_duration 4.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 \u2014 5-Fold CV (Model Selection)\nEdit `REPRESENTATION` and `BACKBONE`. This is the main selection step and can take time."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Default: 5-fold CV for model selection\n",
    "REPRESENTATION = 'mfcc'\n",
    "BACKBONE = 'mobilenetv2'\n",
    "TRAIN_DEVICES = None  # e.g. ['iphone'] for within-device CV\n",
    "VAL_DEVICES = None\n",
    "TEST_DEVICES = None\n",
    "NORMALIZATION = 'per_device' if PER_DEVICE_STATS else 'global'\n",
    "\n",
    "AUTO_POS_WEIGHT = True\n",
    "TUNE_THRESHOLD = True\n",
    "AMP = True\n",
    "USE_CACHE = False  # CV uses on-the-fly features unless you precompute cache per fold\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    '-m',\n",
    "    'src.experiments.run_cv',\n",
    "    '--cv_index',\n",
    "    'splits/cv/index.csv',\n",
    "    '--results_dir',\n",
    "    f'{RUNS_DIR}/results',\n",
    "    '--output_dir',\n",
    "    f'{RUNS_DIR}/checkpoints',\n",
    "    '--',\n",
    "    '--representation',\n",
    "    REPRESENTATION,\n",
    "    '--backbone',\n",
    "    BACKBONE,\n",
    "    '--normalization',\n",
    "    NORMALIZATION,\n",
    "]\n",
    "\n",
    "if AUTO_POS_WEIGHT:\n",
    "    cmd.append('--auto_pos_weight')\n",
    "if TUNE_THRESHOLD:\n",
    "    cmd.append('--tune_threshold')\n",
    "if AMP:\n",
    "    cmd.append('--amp')\n",
    "if USE_CACHE:\n",
    "    cmd.append('--use_cache')\n",
    "\n",
    "if TRAIN_DEVICES:\n",
    "    cmd += ['--train_device_filter', *TRAIN_DEVICES]\n",
    "if VAL_DEVICES:\n",
    "    cmd += ['--val_device_filter', *VAL_DEVICES]\n",
    "if TEST_DEVICES:\n",
    "    cmd += ['--test_device_filter', *TEST_DEVICES]\n",
    "\n",
    "print('Running:', ' '.join(cmd))\n",
    "subprocess.run(cmd, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips to Avoid Mistakes\n",
    "- If you change data or filename rules, rerun from Step 3.\n",
    "- Device names must match metadata (`iphone`, `android_phone`, `digital_stethoscope`).\n",
    "- Only set `USE_CACHE=True` if you ran the cache cell.\n",
    "- For cross-device eval, replace `<run_name>` with the actual checkpoint folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 \u2014 Final Within-Device Training\nEnable this after CV to train one final checkpoint per device."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optional: train a final within-device model (single run)\n",
    "RUN_SINGLE = False\n",
    "\n",
    "if RUN_SINGLE:\n",
    "    import sys\n",
    "    import subprocess\n",
    "\n",
    "    REPRESENTATION = 'mfcc'\n",
    "    BACKBONE = 'mobilenetv2'\n",
    "    TRAIN_DEVICES = None  # e.g. ['android_phone']\n",
    "    VAL_DEVICES = None\n",
    "    TEST_DEVICES = None\n",
    "    USE_CACHE = True  # set True only if you ran the caching cell\n",
    "\n",
    "    if USE_CACHE and not RUN_CACHE:\n",
    "        print('Warning: USE_CACHE=True but RUN_CACHE=False. Run the caching cell or set USE_CACHE=False.')\n",
    "    if not USE_CACHE and not os.path.exists('tf_stats.json'):\n",
    "        print('Warning: tf_stats.json not found. Run the stats cell or set RUN_GLOBAL_STATS=True.')\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        '-m',\n",
    "        'src.training.train',\n",
    "        '--train_csv',\n",
    "        'splits/metadata_train.csv',\n",
    "        '--val_csv',\n",
    "        'splits/metadata_val.csv',\n",
    "        '--test_csv',\n",
    "        'splits/metadata_test.csv',\n",
    "        '--representation',\n",
    "        REPRESENTATION,\n",
    "        '--backbone',\n",
    "        BACKBONE,\n",
    "        '--results_dir',\n",
    "        f'{RUNS_DIR}/results',\n",
    "        '--output_dir',\n",
    "        f'{RUNS_DIR}/checkpoints',\n",
    "        '--auto_pos_weight',\n",
    "        '--tune_threshold',\n",
    "        '--amp',\n",
    "        '--per_device_eval',\n",
    "        '--save_predictions',\n",
    "        '--save_history',\n",
    "    ]\n",
    "\n",
    "    if USE_CACHE:\n",
    "        cmd.append('--use_cache')\n",
    "    if TRAIN_DEVICES:\n",
    "        cmd += ['--train_device_filter', *TRAIN_DEVICES]\n",
    "    if VAL_DEVICES:\n",
    "        cmd += ['--val_device_filter', *VAL_DEVICES]\n",
    "    if TEST_DEVICES:\n",
    "        cmd += ['--test_device_filter', *TEST_DEVICES]\n",
    "\n",
    "    print('Running:', ' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 \u2014 Cross-Device Evaluation (No Retraining)\nUses a saved checkpoint and evaluates on other devices. Replace `<run_name>` with the actual folder."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optional: cross-device evaluation from a saved checkpoint (no retraining)\n",
    "RUN_EVAL_ONLY = False\n",
    "\n",
    "if RUN_EVAL_ONLY:\n",
    "    import sys\n",
    "    import subprocess\n",
    "\n",
    "    CHECKPOINT_PATH = f'{RUNS_DIR}/checkpoints/<run_name>/best.pth'\n",
    "    TEST_DEVICES = ['iphone', 'digital_stethoscope']\n",
    "\n",
    "    if not os.path.exists(CHECKPOINT_PATH):\n",
    "        raise FileNotFoundError(f'Checkpoint not found: {CHECKPOINT_PATH}')\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        '-m',\n",
    "        'src.training.train',\n",
    "        '--eval_only',\n",
    "        '--checkpoint_path',\n",
    "        CHECKPOINT_PATH,\n",
    "        '--train_csv',\n",
    "        'splits/metadata_train.csv',\n",
    "        '--val_csv',\n",
    "        'splits/metadata_val.csv',\n",
    "        '--test_csv',\n",
    "        'splits/metadata_test.csv',\n",
    "        '--results_dir',\n",
    "        f'{RUNS_DIR}/results',\n",
    "        '--per_device_eval',\n",
    "        '--save_predictions',\n",
    "    ]\n",
    "\n",
    "    if TEST_DEVICES:\n",
    "        cmd += ['--test_device_filter', *TEST_DEVICES]\n",
    "\n",
    "    print('Running:', ' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 \u2014 Sync Outputs to Drive\nCopies results and (optionally) derived artifacts back to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if SYNC_DERIVED:\n",
    "    if os.path.exists('metadata.csv'):\n",
    "        shutil.copy2('metadata.csv', f\"{DRIVE_REPO_DIR}/metadata.csv\")\n",
    "    if os.path.exists('tf_stats.json'):\n",
    "        shutil.copy2('tf_stats.json', f\"{DRIVE_REPO_DIR}/tf_stats.json\")\n",
    "    if os.path.isdir('splits'):\n",
    "        rsync('splits/', f\"{DRIVE_REPO_DIR}/splits/\")\n",
    "    print('Synced derived artifacts to drive.')\n",
    "\n",
    "if SYNC_BACK_TO_DRIVE:\n",
    "    os.makedirs(DRIVE_RUNS_DIR, exist_ok=True)\n",
    "    rsync(RUNS_DIR + '/', DRIVE_RUNS_DIR + '/')\n",
    "    print('Synced runs to drive:', DRIVE_RUNS_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}