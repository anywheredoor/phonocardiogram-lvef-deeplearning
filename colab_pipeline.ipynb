{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep-Learning Analysis of Smartphone and Electronic-Stethoscope Phonocardiograms for Detection of Reduced Left Ventricular Ejection Fraction\n",
        "\n",
        "This notebook rebuilds derived artifacts and runs experiments on Google Colab.\n",
        "Default settings prioritize fast training (local /content storage for code, data, cache, and results).\n",
        "Set `USE_LOCAL_DATA = False` if your dataset is too large for /content.\n",
        "\n",
        "No deep-learning background is required: follow the steps in order and keep defaults unless you have a reason to change them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Use This Notebook\n",
        "Run cells from top to bottom. Optional steps are controlled by simple flags (e.g., `RUN_CACHE`).\n",
        "If you are unsure, keep the defaults.\n",
        "For pooled experiments, only change `REPRESENTATION` and `BACKBONE`.\n",
        "For within-device experiments, also set `TRAIN_DEVICES`, `VAL_DEVICES`, and `TEST_DEVICES` to the same device (e.g., `['iphone']`).\n",
        "\n",
        "**Default behavior:**\n",
        "- Builds metadata and splits\n",
        "- Runs 5-fold CV for model selection\n",
        "- Skips caching and final/eval-only runs unless you enable them\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Glossary (Quick)\n",
        "- Representation: MFCC or gammatone spectrogram input\n",
        "- Backbone: ImageNet-pretrained model (e.g., MobileNetV2, SwinV2)\n",
        "- Normalization: standardize spectrograms (global or per-device)\n",
        "- CV (5-fold): train 5 times on different splits, average results\n",
        "- Checkpoint: saved model weights (`best.pth`)\n",
        "- Eval-only: evaluate a saved checkpoint without retraining\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0 \u2014 Mount Google Drive\nSet `DRIVE_REPO_DIR` to the folder containing this repo and your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_REPO_DIR = '/content/drive/MyDrive/phonocardiogram-lvef-deeplearning'\n",
        "WORK_DIR = '/content/pcg_repo'\n",
        "DATA_DIR = '/content/pcg_data'\n",
        "RUNS_DIR = '/content/pcg_runs'\n",
        "USE_LOCAL_DATA = True  # fastest default\n",
        "PER_DEVICE_STATS = False  # set True for per-device normalization\n",
        "SYNC_BACK_TO_DRIVE = True\n",
        "SYNC_DERIVED = True\n",
        "SYNC_DELETE = False  # set True to mirror local runs to Drive (deletes Drive-only files)\n",
        "DRIVE_RUNS_DIR = f\"{DRIVE_REPO_DIR}/runs\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 \u2014 Copy Repo/Data to /content (Fast Mode)\nThis copies the repo (and optionally the dataset) to local Colab storage for speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "def rsync(src, dst, excludes=None, delete=False):\n",
        "    cmd = ['rsync', '-a']\n",
        "    if delete:\n",
        "        cmd.append('--delete')\n",
        "    if excludes:\n",
        "        for ex in excludes:\n",
        "            cmd += ['--exclude', ex]\n",
        "    cmd += [src, dst]\n",
        "    print(' '.join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "rsync(DRIVE_REPO_DIR + '/', WORK_DIR + '/', excludes=['.git', 'cache', 'splits', 'results', 'checkpoints', 'checkpoints_cpu', '__pycache__'], delete=True)\n",
        "\n",
        "if USE_LOCAL_DATA:\n",
        "    os.makedirs(DATA_DIR, exist_ok=True)\n",
        "    rsync(DRIVE_REPO_DIR + '/heart_sounds/', DATA_DIR + '/heart_sounds/', delete=True)\n",
        "    shutil.copy2(DRIVE_REPO_DIR + '/lvef.csv', DATA_DIR + '/lvef.csv')\n",
        "    LVEF_CSV = f\"{DATA_DIR}/lvef.csv\"\n",
        "    HEART_DIR = f\"{DATA_DIR}/heart_sounds\"\n",
        "else:\n",
        "    LVEF_CSV = f\"{DRIVE_REPO_DIR}/lvef.csv\"\n",
        "    HEART_DIR = f\"{DRIVE_REPO_DIR}/heart_sounds\"\n",
        "\n",
        "if not os.path.exists(LVEF_CSV):\n",
        "    raise FileNotFoundError(f'Missing LVEF CSV: {LVEF_CSV}')\n",
        "if not os.path.isdir(HEART_DIR):\n",
        "    raise FileNotFoundError(f'Missing heart_sounds dir: {HEART_DIR}')\n",
        "\n",
        "os.makedirs(RUNS_DIR, exist_ok=True)\n",
        "os.chdir(WORK_DIR)\n",
        "print('WORK_DIR:', WORK_DIR)\n",
        "print('LVEF_CSV:', LVEF_CSV)\n",
        "print('HEART_DIR:', HEART_DIR)\n",
        "print('RUNS_DIR:', RUNS_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 \u2014 Install Requirements\nColab already provides PyTorch. This installs the remaining packages from `requirements.txt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "print('torch:', torch.__version__)\n",
        "print('cuda:', torch.version.cuda)\n",
        "print('cuda available:', torch.cuda.is_available())\n",
        "\n",
        "reqs = [r.strip() for r in Path('requirements.txt').read_text().splitlines() if r.strip()]\n",
        "reqs = [r for r in reqs if not r.startswith('torch') and not r.startswith('torchaudio')]\n",
        "cmd = [sys.executable, '-m', 'pip', 'install'] + reqs\n",
        "print('Installing:', ' '.join(reqs))\n",
        "subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check the filename pattern (important)\n",
        "This workflow expects filenames like `aData2001A.wav`, where:\n",
        "- First letter = device code (`a`=android_phone, `i`=iphone, `e`=digital stethoscope)\n",
        "- Digits = patient ID\n",
        "- Last letter = auscultation position (A/P/M/T)\n",
        "\n",
        "If your naming differs, edit `FILENAME_RE` and `DEVICE_MAP` in `src/data/build_metadata.py` before running the next cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 \u2014 Build Metadata\nCreates `metadata.csv` by linking each WAV to patient ID, device, position, and label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!python -m src.data.build_metadata \\\n",
        "  --lvef_csv {LVEF_CSV} \\\n",
        "  --heart_dir {HEART_DIR} \\\n",
        "  --output_csv metadata.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 \u2014 Create Patient-Level Splits\nCreates train/val/test splits and 5-fold CV splits (no patient leakage)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!python -m src.data.make_patient_splits \\\n",
        "  --metadata_csv metadata.csv \\\n",
        "  --output_dir splits\n",
        "\n",
        "!python -m src.data.make_patient_cv_splits \\\n",
        "  --metadata_csv metadata.csv \\\n",
        "  --output_dir splits/cv \\\n",
        "  --n_splits 5 \\\n",
        "  --n_repeats 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional \u2014 Global TF Stats (for caching or non-CV runs)\nSet `RUN_GLOBAL_STATS=True` only if you plan to cache tensors or train without CV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "RUN_GLOBAL_STATS = False  # set True for caching or non-CV training\n",
        "# PER_DEVICE_STATS is set in Step 0\n",
        "\n",
        "if RUN_GLOBAL_STATS:\n",
        "    per_device_flag = '--per_device' if PER_DEVICE_STATS else ''\n",
        "\n",
        "    !python -m src.data.compute_stats \\\n",
        "      --train_csv splits/metadata_train.csv \\\n",
        "      --representations mfcc gammatone \\\n",
        "      {per_device_flag}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional \u2014 Precompute Cache\nSet `RUN_CACHE=True` to build cached tensors for faster single-run training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "RUN_CACHE = False  # enable if you plan to train with --use_cache\n",
        "CACHE_ROOT = '/content/pcg_cache'\n",
        "NORMALIZATION = 'per_device' if PER_DEVICE_STATS else 'global'\n",
        "\n",
        "if RUN_CACHE:\n",
        "    if not os.path.exists('tf_stats.json'):\n",
        "        per_device_flag = '--per_device' if PER_DEVICE_STATS else ''\n",
        "        print('Computing tf_stats.json for caching...')\n",
        "        !python -m src.data.compute_stats \\\n",
        "          --train_csv splits/metadata_train.csv \\\n",
        "          --representations mfcc gammatone \\\n",
        "          {per_device_flag}\n",
        "\n",
        "    for rep in ['mfcc', 'gammatone']:\n",
        "        print(f'Caching {rep}...')\n",
        "        !python -m src.data.precompute_cache \\\n",
        "          --representation {rep} \\\n",
        "          --normalization {NORMALIZATION} \\\n",
        "          --cache_root {CACHE_ROOT} \\\n",
        "          --splits splits/metadata_train.csv splits/metadata_val.csv splits/metadata_test.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional \u2014 QA Report\nUse this to audit audio durations, sample rates, and label sanity checks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optional QA report\n",
        "# !mkdir -p reports\n",
        "# !python -m src.data.qa_report \\\n",
        "#   --metadata_csv metadata.csv \\\n",
        "#   --output_json reports/qa_report.json \\\n",
        "#   --output_csv reports/qa_records.csv \\\n",
        "#   --fixed_duration 4.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 \u2014 5-Fold CV (Model Selection)\n",
        "Edit `REPRESENTATION` and `BACKBONE` and run this cell.\n",
        "Repeat for each configuration you want to compare.\n",
        "This step trains 5 times and summarizes results; use it to pick the best config.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Default: 5-fold CV for model selection\n",
        "REPRESENTATION = 'mfcc'  # change to 'gammatone' to compare\n",
        "BACKBONE = 'mobilenetv2'  # change to another backbone to compare\n",
        "TRAIN_DEVICES = None  # set e.g. ['iphone'] for within-device CV; leave None for pooled\n",
        "VAL_DEVICES = None  # set equal to TRAIN_DEVICES for within-device CV\n",
        "TEST_DEVICES = None  # set equal to TRAIN_DEVICES for within-device CV\n",
        "NORMALIZATION = 'per_device' if PER_DEVICE_STATS else 'global'\n",
        "\n",
        "AUTO_POS_WEIGHT = True\n",
        "TUNE_THRESHOLD = True\n",
        "AMP = True\n",
        "USE_CACHE = False  # leave False for CV unless you built per-fold caches\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    '-m',\n",
        "    'src.experiments.run_cv',\n",
        "    '--cv_index',\n",
        "    'splits/cv/index.csv',\n",
        "    '--results_dir',\n",
        "    f'{RUNS_DIR}/results',\n",
        "    '--output_dir',\n",
        "    f'{RUNS_DIR}/checkpoints',\n",
        "    '--',\n",
        "    '--representation',\n",
        "    REPRESENTATION,\n",
        "    '--backbone',\n",
        "    BACKBONE,\n",
        "    '--normalization',\n",
        "    NORMALIZATION,\n",
        "]\n",
        "\n",
        "if AUTO_POS_WEIGHT:\n",
        "    cmd.append('--auto_pos_weight')\n",
        "if TUNE_THRESHOLD:\n",
        "    cmd.append('--tune_threshold')\n",
        "if AMP:\n",
        "    cmd.append('--amp')\n",
        "if USE_CACHE:\n",
        "    cmd.append('--use_cache')\n",
        "\n",
        "if TRAIN_DEVICES:\n",
        "    cmd += ['--train_device_filter', *TRAIN_DEVICES]\n",
        "if VAL_DEVICES:\n",
        "    cmd += ['--val_device_filter', *VAL_DEVICES]\n",
        "if TEST_DEVICES:\n",
        "    cmd += ['--test_device_filter', *TEST_DEVICES]\n",
        "\n",
        "print('Running:', ' '.join(cmd))\n",
        "subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tips to Avoid Mistakes\n",
        "- If you change data or filename rules, rerun from Step 3.\n",
        "- Device names must match metadata (`iphone`, `android_phone`, `digital_stethoscope`). If your labels differ (e.g., `android`), update `DEVICE_MAP` in `src/data/build_metadata.py` and rebuild metadata.\n",
        "- Only set `USE_CACHE=True` if you ran the cache cell.\n",
        "- For cross-device eval, replace `<run_name>` with the actual checkpoint folder.\n",
        "- Do not tune anything using the test split.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6 \u2014 Final Within-Device Training\n",
        "Enable this only after CV. This creates one final checkpoint per device using the chosen config.\n",
        "This step requires `tf_stats.json` (run the Optional Global TF Stats or Cache step first).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optional: train a final within-device model (single run)\n",
        "RUN_SINGLE = False  # set True after you pick the best config from CV\n",
        "\n",
        "if RUN_SINGLE:\n",
        "    import sys\n",
        "    import subprocess\n",
        "\n",
        "    REPRESENTATION = 'mfcc'\n",
        "    BACKBONE = 'mobilenetv2'\n",
        "    TRAIN_DEVICES = None  # set to one device for within-device final model\n",
        "    VAL_DEVICES = None  # set equal to TRAIN_DEVICES for within-device\n",
        "    TEST_DEVICES = None  # set equal to TRAIN_DEVICES for within-device\n",
        "    NORMALIZATION = 'per_device' if PER_DEVICE_STATS else 'global'\n",
        "    USE_CACHE = False  # set True only if you ran the caching cell\n",
        "\n",
        "    if USE_CACHE and not RUN_CACHE:\n",
        "        print('Warning: USE_CACHE=True but RUN_CACHE=False. Run the caching cell or set USE_CACHE=False.')\n",
        "    if not USE_CACHE and not os.path.exists('tf_stats.json'):\n",
        "        raise FileNotFoundError('tf_stats.json not found. Run the Optional Global TF Stats or Cache step first.')\n",
        "\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        '-m',\n",
        "        'src.training.train',\n",
        "        '--train_csv',\n",
        "        'splits/metadata_train.csv',\n",
        "        '--val_csv',\n",
        "        'splits/metadata_val.csv',\n",
        "        '--test_csv',\n",
        "        'splits/metadata_test.csv',\n",
        "        '--representation',\n",
        "        REPRESENTATION,\n",
        "        '--backbone',\n",
        "        BACKBONE,\n",
        "        '--normalization',\n",
        "        NORMALIZATION,\n",
        "        '--results_dir',\n",
        "        f'{RUNS_DIR}/results',\n",
        "        '--output_dir',\n",
        "        f'{RUNS_DIR}/checkpoints',\n",
        "        '--auto_pos_weight',\n",
        "        '--tune_threshold',\n",
        "        '--amp',\n",
        "        '--per_device_eval',\n",
        "        '--save_predictions',\n",
        "        '--save_history',\n",
        "    ]\n",
        "\n",
        "    if USE_CACHE:\n",
        "        cmd.append('--use_cache')\n",
        "    if TRAIN_DEVICES:\n",
        "        cmd += ['--train_device_filter', *TRAIN_DEVICES]\n",
        "    if VAL_DEVICES:\n",
        "        cmd += ['--val_device_filter', *VAL_DEVICES]\n",
        "    if TEST_DEVICES:\n",
        "        cmd += ['--test_device_filter', *TEST_DEVICES]\n",
        "\n",
        "    print('Running:', ' '.join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7 \u2014 Cross-Device Evaluation (No Retraining)\n",
        "Uses a saved checkpoint and evaluates on other devices. The model is not updated.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optional: cross-device evaluation from a saved checkpoint (no retraining)\n",
        "RUN_EVAL_ONLY = False\n",
        "\n",
        "if RUN_EVAL_ONLY:\n",
        "    import sys\n",
        "    import subprocess\n",
        "\n",
        "    CHECKPOINT_PATH = f'{RUNS_DIR}/checkpoints/<run_name>/best.pth'\n",
        "    TEST_DEVICES = ['iphone', 'digital_stethoscope']  # set target device(s); use metadata labels\n",
        "\n",
        "    if not os.path.exists(CHECKPOINT_PATH):\n",
        "        raise FileNotFoundError(f'Checkpoint not found: {CHECKPOINT_PATH}')\n",
        "\n",
        "    # Representation/backbone/normalization are loaded from the checkpoint.\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        '-m',\n",
        "        'src.training.train',\n",
        "        '--eval_only',\n",
        "        '--checkpoint_path',\n",
        "        CHECKPOINT_PATH,\n",
        "        '--train_csv',\n",
        "        'splits/metadata_train.csv',\n",
        "        '--val_csv',\n",
        "        'splits/metadata_val.csv',\n",
        "        '--test_csv',\n",
        "        'splits/metadata_test.csv',\n",
        "        '--results_dir',\n",
        "        f'{RUNS_DIR}/results',\n",
        "        '--per_device_eval',\n",
        "        '--save_predictions',\n",
        "    ]\n",
        "\n",
        "    # If the checkpoint was trained with cached tensors, those cache files must exist.\n",
        "    if TEST_DEVICES:\n",
        "        cmd += ['--test_device_filter', *TEST_DEVICES]\n",
        "\n",
        "    print('Running:', ' '.join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outputs to Inspect\n",
        "All outputs are under `RUNS_DIR` (default: `/content/pcg_runs`):\n",
        "- `RUNS_DIR/results/summary.csv`: main table of metrics per run (use this for tables)\n",
        "- `RUNS_DIR/results/<run_name>/predictions_test.csv`: for ROC/PR curves (final models only)\n",
        "- `RUNS_DIR/checkpoints/<run_name>/best.pth`: saved model weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8 \u2014 Sync Outputs to Drive\nCopies results and (optionally) derived artifacts back to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if SYNC_DERIVED:\n",
        "    if os.path.exists('metadata.csv'):\n",
        "        shutil.copy2('metadata.csv', f\"{DRIVE_REPO_DIR}/metadata.csv\")\n",
        "    if os.path.exists('tf_stats.json'):\n",
        "        shutil.copy2('tf_stats.json', f\"{DRIVE_REPO_DIR}/tf_stats.json\")\n",
        "    if os.path.isdir('splits'):\n",
        "        rsync('splits/', f\"{DRIVE_REPO_DIR}/splits/\", delete=SYNC_DELETE)\n",
        "    print('Synced derived artifacts to drive.')\n",
        "\n",
        "if SYNC_BACK_TO_DRIVE:\n",
        "    os.makedirs(DRIVE_RUNS_DIR, exist_ok=True)\n",
        "    rsync(RUNS_DIR + '/', DRIVE_RUNS_DIR + '/', delete=SYNC_DELETE)\n",
        "    print('Synced runs to drive:', DRIVE_RUNS_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}